{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19643619-7ef9-45ac-b5f8-0b3993b63816",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from io import open\n",
    "import argparse\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efaf25ae-cbc5-423a-9c73-d583c064b848",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = []\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.idx2word.append(word)\n",
    "            self.word2idx[word] = len(self.idx2word) - 1\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "849950a7-0f4b-479b-84a7-0d150bbdac8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus(object):\n",
    "    def __init__(self, path):\n",
    "        self.dictionary = Dictionary()\n",
    "        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n",
    "        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n",
    "        self.test = self.tokenize(os.path.join(path, 'test.txt'))\n",
    "\n",
    "    def tokenize(self, path):\n",
    "        \"\"\"Tokenizes a text file.\"\"\"\n",
    "        assert os.path.exists(path)\n",
    "        # Add words to the dictionary\n",
    "        with open(path, 'r', encoding=\"utf8\") as f:\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                for word in words:\n",
    "                    self.dictionary.add_word(word)\n",
    "\n",
    "        # Tokenize file content\n",
    "        with open(path, 'r', encoding=\"utf8\") as f:\n",
    "            idss = []\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                ids = []\n",
    "                for word in words:\n",
    "                    ids.append(self.dictionary.word2idx[word])\n",
    "                idss.append(torch.tensor(ids).type(torch.int64))\n",
    "            ids = torch.cat(idss)\n",
    "\n",
    "        return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9acbb704-6f3f-4ba3-87c2-8dd982082d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=0.5, tie_weights=False):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.ntoken = ntoken\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        if rnn_type in ['LSTM', 'GRU']:\n",
    "            self.rnn = getattr(nn, rnn_type)(ninp, nhid, nlayers, dropout=dropout)\n",
    "        else:\n",
    "            try:\n",
    "                nonlinearity = {'RNN_TANH': 'tanh', 'RNN_RELU': 'relu'}[rnn_type]\n",
    "            except KeyError as e:\n",
    "                raise ValueError( \"\"\"An invalid option for `--model` was supplied,\n",
    "                                 options are ['LSTM', 'GRU', 'RNN_TANH' or 'RNN_RELU']\"\"\") from e\n",
    "            self.rnn = nn.RNN(ninp, nhid, nlayers, nonlinearity=nonlinearity, dropout=dropout)\n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "\n",
    "        # Optionally tie weights as in:\n",
    "        # \"Using the Output Embedding to Improve Language Models\" (Press & Wolf 2016)\n",
    "        # https://arxiv.org/abs/1608.05859\n",
    "        # and\n",
    "        # \"Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling\" (Inan et al. 2016)\n",
    "        # https://arxiv.org/abs/1611.01462\n",
    "        if tie_weights:\n",
    "            if nhid != ninp:\n",
    "                raise ValueError('When using the tied flag, nhid must be equal to emsize')\n",
    "            self.decoder.weight = self.encoder.weight\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        self.rnn_type = rnn_type\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        nn.init.uniform_(self.encoder.weight, -initrange, initrange)\n",
    "        nn.init.zeros_(self.decoder.bias)\n",
    "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        emb = self.drop(self.encoder(input))\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output)\n",
    "        decoded = decoded.view(-1, self.ntoken)\n",
    "        return F.log_softmax(decoded, dim=1), hidden\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters())\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            return (weight.new_zeros(self.nlayers, bsz, self.nhid),\n",
    "                    weight.new_zeros(self.nlayers, bsz, self.nhid))\n",
    "        else:\n",
    "            return weight.new_zeros(self.nlayers, bsz, self.nhid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3f0857a-ed15-460b-b2ec-7b7320175a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Transformer):\n",
    "    \"\"\"Container module with an encoder, a recurrent or transformer module, and a decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__(d_model=ninp, nhead=nhead, dim_feedforward=nhid, num_encoder_layers=nlayers)\n",
    "        self.model_type = 'Transformer'\n",
    "        self.src_mask = None\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "\n",
    "        self.input_emb = nn.Embedding(ntoken, ninp)\n",
    "        self.ninp = ninp\n",
    "        self.decoder = nn.Linear(ninp, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        return torch.log(torch.tril(torch.ones(sz,sz)))\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        nn.init.uniform_(self.input_emb.weight, -initrange, initrange)\n",
    "        nn.init.zeros_(self.decoder.bias)\n",
    "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
    "\n",
    "    def forward(self, src, has_mask=True):\n",
    "        if has_mask:\n",
    "            device = src.device\n",
    "            if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "                mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "                self.src_mask = mask\n",
    "        else:\n",
    "            self.src_mask = None\n",
    "\n",
    "        src = self.input_emb(src) * math.sqrt(self.ninp)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.encoder(src, mask=self.src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return F.log_softmax(output, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "517e00b3-5445-4909-9fbb-4d856f40783b",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch Wikitext-2 RNN/LSTM/GRU/Transformer Language Model')\n",
    "parser.add_argument('--data', type=str, default='./data/wikitext-2',\n",
    "                    help='location of the data corpus')\n",
    "parser.add_argument('--model', type=str, default='LSTM',\n",
    "                    help='type of network (RNN_TANH, RNN_RELU, LSTM, GRU, Transformer)')\n",
    "parser.add_argument('--emsize', type=int, default=200,\n",
    "                    help='size of word embeddings')\n",
    "parser.add_argument('--nhid', type=int, default=200,\n",
    "                    help='number of hidden units per layer')\n",
    "parser.add_argument('--nlayers', type=int, default=2,\n",
    "                    help='number of layers')\n",
    "parser.add_argument('--lr', type=float, default=20,\n",
    "                    help='initial learning rate')\n",
    "parser.add_argument('--clip', type=float, default=0.25,\n",
    "                    help='gradient clipping')\n",
    "parser.add_argument('--epochs', type=int, default=40,\n",
    "                    help='upper epoch limit')\n",
    "parser.add_argument('--batch_size', type=int, default=20, metavar='N',\n",
    "                    help='batch size')\n",
    "parser.add_argument('--bptt', type=int, default=35,\n",
    "                    help='sequence length')\n",
    "parser.add_argument('--dropout', type=float, default=0.2,\n",
    "                    help='dropout applied to layers (0 = no dropout)')\n",
    "parser.add_argument('--tied', action='store_true',\n",
    "                    help='tie the word embedding and softmax weights')\n",
    "parser.add_argument('--seed', type=int, default=1111,\n",
    "                    help='random seed')\n",
    "parser.add_argument('--cuda', action='store_true', default=True,\n",
    "                    help='use CUDA') # should set default to false later\n",
    "parser.add_argument('--mps', action='store_true', default=False,\n",
    "                        help='enables macOS GPU training')\n",
    "parser.add_argument('--log-interval', type=int, default=200, metavar='N',\n",
    "                    help='report interval')\n",
    "parser.add_argument('--save', type=str, default='model.pt',\n",
    "                    help='path to save the final model')\n",
    "parser.add_argument('--onnx-export', type=str, default='',\n",
    "                    help='path to export the final model in onnx format')\n",
    "parser.add_argument('--nhead', type=int, default=2,\n",
    "                    help='the number of heads in the encoder/decoder of the transformer model')\n",
    "parser.add_argument('--dry-run', action='store_true',\n",
    "                    help='verify the code and the model')\n",
    "args = parser.parse_args(args=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23b967ca-7677-4981-a910-e44ae314110b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed manually for reproducibility.\n",
    "torch.manual_seed(args.seed)\n",
    "if torch.cuda.is_available():\n",
    "    if not args.cuda:\n",
    "        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda.\")\n",
    "if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    if not args.mps:\n",
    "        print(\"WARNING: You have mps device, to enable macOS GPU run with --mps.\")\n",
    "\n",
    "use_mps = args.mps and torch.backends.mps.is_available()\n",
    "if args.cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "elif use_mps:\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4aef637-0e50-4962-ba29-973c1bf32c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpus(args.data)\n",
    "\n",
    "def batchify(data, bsz):\n",
    "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "eval_batch_size = 10\n",
    "train_data = batchify(corpus.train, args.batch_size)\n",
    "val_data = batchify(corpus.valid, eval_batch_size)\n",
    "test_data = batchify(corpus.test, eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae2dea10-eb31-4fd3-885c-dc8b4971b6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(corpus.dictionary)\n",
    "if args.model == 'Transformer':\n",
    "    model = TransformerModel(ntokens, args.emsize, args.nhead, args.nhid, args.nlayers, args.dropout).to(device)\n",
    "else:\n",
    "    model = RNNModel(args.model, ntokens, args.emsize, args.nhid, args.nlayers, args.dropout, args.tied).to(device)\n",
    "\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90d8a8d3-f5f3-4e46-b559-e7cb465fcbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
    "\n",
    "    if isinstance(h, torch.Tensor):\n",
    "        return h.detach()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8363e8c-87ca-4b01-88a7-5d8072492bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(source, i):\n",
    "    seq_len = min(args.bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].view(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b49e85d2-2d65-4e28-b2c5-34529225afd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_source):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    if args.model != 'Transformer':\n",
    "        hidden = model.init_hidden(eval_batch_size)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, args.bptt):\n",
    "            data, targets = get_batch(data_source, i)\n",
    "            if args.model == 'Transformer':\n",
    "                output = model(data)\n",
    "                output = output.view(-1, ntokens)\n",
    "            else:\n",
    "                output, hidden = model(data, hidden)\n",
    "                hidden = repackage_hidden(hidden)\n",
    "            total_loss += len(data) * criterion(output, targets).item()\n",
    "    return total_loss / (len(data_source) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7b6ef07-e9f2-4f70-be57-89e831880351",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Turn on training mode which enables dropout.\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    if args.model != 'Transformer':\n",
    "        hidden = model.init_hidden(args.batch_size)\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, args.bptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
    "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
    "        model.zero_grad()\n",
    "        if args.model == 'Transformer':\n",
    "            output = model(data)\n",
    "            output = output.view(-1, ntokens)\n",
    "        else:\n",
    "            hidden = repackage_hidden(hidden)\n",
    "            output, hidden = model(data, hidden)\n",
    "        loss = criterion(output, targets)\n",
    "        loss.backward()\n",
    "\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
    "        for p in model.parameters():\n",
    "            p.data.add_(p.grad, alpha=-lr)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch % args.log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / args.log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, batch, len(train_data) // args.bptt, lr,\n",
    "                elapsed * 1000 / args.log_interval, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "        if args.dry_run:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "134eec2c-95fe-4592-9c4b-83d63b86d906",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_onnx(path, batch_size, seq_len):\n",
    "    print('The model is also exported in ONNX format at {}.'.format(os.path.realpath(args.onnx_export)))\n",
    "    model.eval()\n",
    "    dummy_input = torch.LongTensor(seq_len * batch_size).zero_().view(-1, batch_size).to(device)\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    torch.onnx.export(model, (dummy_input, hidden), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8fc9c438-fd83-4eaf-8dcb-a1c245c33c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 2983 batches | lr 20.00 | ms/batch  9.54 | loss  7.62 | ppl  2028.76\n",
      "| epoch   1 |   400/ 2983 batches | lr 20.00 | ms/batch  8.77 | loss  6.84 | ppl   935.28\n",
      "| epoch   1 |   600/ 2983 batches | lr 20.00 | ms/batch  8.52 | loss  6.47 | ppl   643.93\n",
      "| epoch   1 |   800/ 2983 batches | lr 20.00 | ms/batch  8.85 | loss  6.29 | ppl   537.08\n",
      "| epoch   1 |  1000/ 2983 batches | lr 20.00 | ms/batch  8.50 | loss  6.13 | ppl   458.86\n",
      "| epoch   1 |  1200/ 2983 batches | lr 20.00 | ms/batch  8.52 | loss  6.05 | ppl   424.85\n",
      "| epoch   1 |  1400/ 2983 batches | lr 20.00 | ms/batch  8.43 | loss  5.95 | ppl   382.18\n",
      "| epoch   1 |  1600/ 2983 batches | lr 20.00 | ms/batch  8.34 | loss  5.95 | ppl   382.57\n",
      "| epoch   1 |  1800/ 2983 batches | lr 20.00 | ms/batch  8.45 | loss  5.81 | ppl   332.05\n",
      "| epoch   1 |  2000/ 2983 batches | lr 20.00 | ms/batch  8.57 | loss  5.78 | ppl   324.57\n",
      "| epoch   1 |  2200/ 2983 batches | lr 20.00 | ms/batch  8.27 | loss  5.67 | ppl   289.63\n",
      "| epoch   1 |  2400/ 2983 batches | lr 20.00 | ms/batch  8.21 | loss  5.67 | ppl   290.66\n",
      "| epoch   1 |  2600/ 2983 batches | lr 20.00 | ms/batch  8.15 | loss  5.66 | ppl   286.14\n",
      "| epoch   1 |  2800/ 2983 batches | lr 20.00 | ms/batch  8.39 | loss  5.53 | ppl   253.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 26.89s | valid loss  5.53 | valid ppl   252.38\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 2983 batches | lr 20.00 | ms/batch  8.32 | loss  5.54 | ppl   255.16\n",
      "| epoch   2 |   400/ 2983 batches | lr 20.00 | ms/batch  8.35 | loss  5.52 | ppl   250.82\n",
      "| epoch   2 |   600/ 2983 batches | lr 20.00 | ms/batch  8.42 | loss  5.35 | ppl   211.16\n",
      "| epoch   2 |   800/ 2983 batches | lr 20.00 | ms/batch  8.41 | loss  5.37 | ppl   214.81\n",
      "| epoch   2 |  1000/ 2983 batches | lr 20.00 | ms/batch  8.17 | loss  5.35 | ppl   209.67\n",
      "| epoch   2 |  1200/ 2983 batches | lr 20.00 | ms/batch  8.33 | loss  5.33 | ppl   205.52\n",
      "| epoch   2 |  1400/ 2983 batches | lr 20.00 | ms/batch  8.78 | loss  5.32 | ppl   204.73\n",
      "| epoch   2 |  1600/ 2983 batches | lr 20.00 | ms/batch  9.65 | loss  5.38 | ppl   217.69\n",
      "| epoch   2 |  1800/ 2983 batches | lr 20.00 | ms/batch  8.72 | loss  5.26 | ppl   191.83\n",
      "| epoch   2 |  2000/ 2983 batches | lr 20.00 | ms/batch  8.10 | loss  5.26 | ppl   193.36\n",
      "| epoch   2 |  2200/ 2983 batches | lr 20.00 | ms/batch  8.66 | loss  5.17 | ppl   175.33\n",
      "| epoch   2 |  2400/ 2983 batches | lr 20.00 | ms/batch  8.82 | loss  5.20 | ppl   181.65\n",
      "| epoch   2 |  2600/ 2983 batches | lr 20.00 | ms/batch  8.81 | loss  5.22 | ppl   184.74\n",
      "| epoch   2 |  2800/ 2983 batches | lr 20.00 | ms/batch  9.11 | loss  5.12 | ppl   167.64\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 27.33s | valid loss  5.28 | valid ppl   196.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 2983 batches | lr 20.00 | ms/batch  8.98 | loss  5.18 | ppl   178.18\n",
      "| epoch   3 |   400/ 2983 batches | lr 20.00 | ms/batch  8.71 | loss  5.20 | ppl   180.76\n",
      "| epoch   3 |   600/ 2983 batches | lr 20.00 | ms/batch  8.93 | loss  5.02 | ppl   151.51\n",
      "| epoch   3 |   800/ 2983 batches | lr 20.00 | ms/batch  8.57 | loss  5.06 | ppl   157.90\n",
      "| epoch   3 |  1000/ 2983 batches | lr 20.00 | ms/batch  8.54 | loss  5.05 | ppl   156.62\n",
      "| epoch   3 |  1200/ 2983 batches | lr 20.00 | ms/batch  8.63 | loss  5.05 | ppl   155.83\n",
      "| epoch   3 |  1400/ 2983 batches | lr 20.00 | ms/batch  8.32 | loss  5.07 | ppl   159.04\n",
      "| epoch   3 |  1600/ 2983 batches | lr 20.00 | ms/batch  8.41 | loss  5.14 | ppl   171.13\n",
      "| epoch   3 |  1800/ 2983 batches | lr 20.00 | ms/batch  8.48 | loss  5.02 | ppl   151.16\n",
      "| epoch   3 |  2000/ 2983 batches | lr 20.00 | ms/batch  8.74 | loss  5.04 | ppl   154.70\n",
      "| epoch   3 |  2200/ 2983 batches | lr 20.00 | ms/batch  8.92 | loss  4.94 | ppl   140.30\n",
      "| epoch   3 |  2400/ 2983 batches | lr 20.00 | ms/batch  8.37 | loss  4.99 | ppl   146.62\n",
      "| epoch   3 |  2600/ 2983 batches | lr 20.00 | ms/batch  8.29 | loss  5.01 | ppl   150.07\n",
      "| epoch   3 |  2800/ 2983 batches | lr 20.00 | ms/batch  9.14 | loss  4.93 | ppl   138.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 27.64s | valid loss  5.16 | valid ppl   174.87\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |   200/ 2983 batches | lr 20.00 | ms/batch  8.24 | loss  4.99 | ppl   146.67\n",
      "| epoch   4 |   400/ 2983 batches | lr 20.00 | ms/batch  8.15 | loss  5.02 | ppl   151.11\n",
      "| epoch   4 |   600/ 2983 batches | lr 20.00 | ms/batch  8.37 | loss  4.84 | ppl   126.11\n",
      "| epoch   4 |   800/ 2983 batches | lr 20.00 | ms/batch  8.38 | loss  4.89 | ppl   132.31\n",
      "| epoch   4 |  1000/ 2983 batches | lr 20.00 | ms/batch  8.17 | loss  4.89 | ppl   132.37\n",
      "| epoch   4 |  1200/ 2983 batches | lr 20.00 | ms/batch  8.19 | loss  4.89 | ppl   133.11\n",
      "| epoch   4 |  1400/ 2983 batches | lr 20.00 | ms/batch  8.21 | loss  4.92 | ppl   136.85\n",
      "| epoch   4 |  1600/ 2983 batches | lr 20.00 | ms/batch  7.97 | loss  4.99 | ppl   147.41\n",
      "| epoch   4 |  1800/ 2983 batches | lr 20.00 | ms/batch  7.99 | loss  4.87 | ppl   130.69\n",
      "| epoch   4 |  2000/ 2983 batches | lr 20.00 | ms/batch  8.13 | loss  4.90 | ppl   134.41\n",
      "| epoch   4 |  2200/ 2983 batches | lr 20.00 | ms/batch  9.19 | loss  4.80 | ppl   121.62\n",
      "| epoch   4 |  2400/ 2983 batches | lr 20.00 | ms/batch  8.24 | loss  4.85 | ppl   127.69\n",
      "| epoch   4 |  2600/ 2983 batches | lr 20.00 | ms/batch  8.71 | loss  4.87 | ppl   130.85\n",
      "| epoch   4 |  2800/ 2983 batches | lr 20.00 | ms/batch  8.30 | loss  4.80 | ppl   121.35\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 26.19s | valid loss  5.12 | valid ppl   167.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |   200/ 2983 batches | lr 20.00 | ms/batch  8.86 | loss  4.87 | ppl   129.98\n",
      "| epoch   5 |   400/ 2983 batches | lr 20.00 | ms/batch  9.91 | loss  4.89 | ppl   133.54\n",
      "| epoch   5 |   600/ 2983 batches | lr 20.00 | ms/batch  8.40 | loss  4.71 | ppl   111.11\n",
      "| epoch   5 |   800/ 2983 batches | lr 20.00 | ms/batch  8.45 | loss  4.77 | ppl   117.54\n",
      "| epoch   5 |  1000/ 2983 batches | lr 20.00 | ms/batch  8.50 | loss  4.77 | ppl   118.50\n",
      "| epoch   5 |  1200/ 2983 batches | lr 20.00 | ms/batch  8.22 | loss  4.78 | ppl   118.88\n",
      "| epoch   5 |  1400/ 2983 batches | lr 20.00 | ms/batch  8.23 | loss  4.82 | ppl   124.15\n",
      "| epoch   5 |  1600/ 2983 batches | lr 20.00 | ms/batch  8.96 | loss  4.89 | ppl   133.09\n",
      "| epoch   5 |  1800/ 2983 batches | lr 20.00 | ms/batch  8.78 | loss  4.77 | ppl   118.30\n",
      "| epoch   5 |  2000/ 2983 batches | lr 20.00 | ms/batch  8.44 | loss  4.80 | ppl   122.02\n",
      "| epoch   5 |  2200/ 2983 batches | lr 20.00 | ms/batch  8.53 | loss  4.70 | ppl   110.12\n",
      "| epoch   5 |  2400/ 2983 batches | lr 20.00 | ms/batch  8.29 | loss  4.75 | ppl   115.30\n",
      "| epoch   5 |  2600/ 2983 batches | lr 20.00 | ms/batch  8.87 | loss  4.77 | ppl   118.08\n",
      "| epoch   5 |  2800/ 2983 batches | lr 20.00 | ms/batch  8.44 | loss  4.70 | ppl   109.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 27.54s | valid loss  5.05 | valid ppl   156.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |   200/ 2983 batches | lr 20.00 | ms/batch  8.45 | loss  4.76 | ppl   117.28\n",
      "| epoch   6 |   400/ 2983 batches | lr 20.00 | ms/batch  8.39 | loss  4.80 | ppl   121.18\n",
      "| epoch   6 |   600/ 2983 batches | lr 20.00 | ms/batch  8.31 | loss  4.62 | ppl   101.83\n",
      "| epoch   6 |   800/ 2983 batches | lr 20.00 | ms/batch  8.10 | loss  4.67 | ppl   106.86\n",
      "| epoch   6 |  1000/ 2983 batches | lr 20.00 | ms/batch  8.09 | loss  4.69 | ppl   108.67\n",
      "| epoch   6 |  1200/ 2983 batches | lr 20.00 | ms/batch  8.15 | loss  4.70 | ppl   109.50\n",
      "| epoch   6 |  1400/ 2983 batches | lr 20.00 | ms/batch  8.28 | loss  4.73 | ppl   113.86\n",
      "| epoch   6 |  1600/ 2983 batches | lr 20.00 | ms/batch  8.01 | loss  4.81 | ppl   122.78\n",
      "| epoch   6 |  1800/ 2983 batches | lr 20.00 | ms/batch  8.35 | loss  4.69 | ppl   108.75\n",
      "| epoch   6 |  2000/ 2983 batches | lr 20.00 | ms/batch  8.31 | loss  4.72 | ppl   112.30\n",
      "| epoch   6 |  2200/ 2983 batches | lr 20.00 | ms/batch  8.00 | loss  4.62 | ppl   101.33\n",
      "| epoch   6 |  2400/ 2983 batches | lr 20.00 | ms/batch  8.30 | loss  4.66 | ppl   106.06\n",
      "| epoch   6 |  2600/ 2983 batches | lr 20.00 | ms/batch  8.09 | loss  4.69 | ppl   109.36\n",
      "| epoch   6 |  2800/ 2983 batches | lr 20.00 | ms/batch  8.28 | loss  4.62 | ppl   101.87\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 26.19s | valid loss  5.03 | valid ppl   153.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 |   200/ 2983 batches | lr 20.00 | ms/batch  8.73 | loss  4.69 | ppl   108.77\n",
      "| epoch   7 |   400/ 2983 batches | lr 20.00 | ms/batch  8.82 | loss  4.71 | ppl   111.60\n",
      "| epoch   7 |   600/ 2983 batches | lr 20.00 | ms/batch  8.14 | loss  4.54 | ppl    94.00\n",
      "| epoch   7 |   800/ 2983 batches | lr 20.00 | ms/batch  8.16 | loss  4.60 | ppl    99.52\n",
      "| epoch   7 |  1000/ 2983 batches | lr 20.00 | ms/batch  8.62 | loss  4.62 | ppl   101.07\n",
      "| epoch   7 |  1200/ 2983 batches | lr 20.00 | ms/batch  8.35 | loss  4.63 | ppl   102.10\n",
      "| epoch   7 |  1400/ 2983 batches | lr 20.00 | ms/batch  8.13 | loss  4.66 | ppl   105.96\n",
      "| epoch   7 |  1600/ 2983 batches | lr 20.00 | ms/batch  8.70 | loss  4.74 | ppl   113.93\n",
      "| epoch   7 |  1800/ 2983 batches | lr 20.00 | ms/batch  8.76 | loss  4.63 | ppl   102.44\n",
      "| epoch   7 |  2000/ 2983 batches | lr 20.00 | ms/batch  8.69 | loss  4.66 | ppl   105.17\n",
      "| epoch   7 |  2200/ 2983 batches | lr 20.00 | ms/batch  8.12 | loss  4.55 | ppl    94.92\n",
      "| epoch   7 |  2400/ 2983 batches | lr 20.00 | ms/batch  8.14 | loss  4.60 | ppl    99.93\n",
      "| epoch   7 |  2600/ 2983 batches | lr 20.00 | ms/batch  8.14 | loss  4.63 | ppl   102.73\n",
      "| epoch   7 |  2800/ 2983 batches | lr 20.00 | ms/batch  8.28 | loss  4.56 | ppl    95.79\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 26.44s | valid loss  5.00 | valid ppl   149.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   8 |   200/ 2983 batches | lr 20.00 | ms/batch  8.27 | loss  4.63 | ppl   102.35\n",
      "| epoch   8 |   400/ 2983 batches | lr 20.00 | ms/batch  8.17 | loss  4.66 | ppl   105.51\n",
      "| epoch   8 |   600/ 2983 batches | lr 20.00 | ms/batch  7.96 | loss  4.49 | ppl    88.87\n",
      "| epoch   8 |   800/ 2983 batches | lr 20.00 | ms/batch  8.15 | loss  4.55 | ppl    94.30\n",
      "| epoch   8 |  1000/ 2983 batches | lr 20.00 | ms/batch  8.05 | loss  4.56 | ppl    95.53\n",
      "| epoch   8 |  1200/ 2983 batches | lr 20.00 | ms/batch  8.09 | loss  4.57 | ppl    96.60\n",
      "| epoch   8 |  1400/ 2983 batches | lr 20.00 | ms/batch  8.48 | loss  4.61 | ppl   100.31\n",
      "| epoch   8 |  1600/ 2983 batches | lr 20.00 | ms/batch  8.47 | loss  4.69 | ppl   108.36\n",
      "| epoch   8 |  1800/ 2983 batches | lr 20.00 | ms/batch  8.44 | loss  4.57 | ppl    96.76\n",
      "| epoch   8 |  2000/ 2983 batches | lr 20.00 | ms/batch  8.48 | loss  4.60 | ppl    99.97\n",
      "| epoch   8 |  2200/ 2983 batches | lr 20.00 | ms/batch  8.36 | loss  4.49 | ppl    89.53\n",
      "| epoch   8 |  2400/ 2983 batches | lr 20.00 | ms/batch  8.51 | loss  4.54 | ppl    93.69\n",
      "| epoch   8 |  2600/ 2983 batches | lr 20.00 | ms/batch  8.60 | loss  4.57 | ppl    96.89\n",
      "| epoch   8 |  2800/ 2983 batches | lr 20.00 | ms/batch  8.24 | loss  4.51 | ppl    90.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 26.27s | valid loss  4.99 | valid ppl   146.65\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   9 |   200/ 2983 batches | lr 20.00 | ms/batch  8.22 | loss  4.58 | ppl    97.22\n",
      "| epoch   9 |   400/ 2983 batches | lr 20.00 | ms/batch  8.47 | loss  4.60 | ppl    99.51\n",
      "| epoch   9 |   600/ 2983 batches | lr 20.00 | ms/batch  9.23 | loss  4.43 | ppl    83.62\n",
      "| epoch   9 |   800/ 2983 batches | lr 20.00 | ms/batch  8.60 | loss  4.49 | ppl    89.29\n",
      "| epoch   9 |  1000/ 2983 batches | lr 20.00 | ms/batch  8.04 | loss  4.51 | ppl    91.01\n",
      "| epoch   9 |  1200/ 2983 batches | lr 20.00 | ms/batch  7.96 | loss  4.52 | ppl    92.21\n",
      "| epoch   9 |  1400/ 2983 batches | lr 20.00 | ms/batch  8.55 | loss  4.56 | ppl    95.63\n",
      "| epoch   9 |  1600/ 2983 batches | lr 20.00 | ms/batch  8.54 | loss  4.64 | ppl   103.07\n",
      "| epoch   9 |  1800/ 2983 batches | lr 20.00 | ms/batch  8.76 | loss  4.52 | ppl    92.14\n",
      "| epoch   9 |  2000/ 2983 batches | lr 20.00 | ms/batch  8.23 | loss  4.56 | ppl    95.83\n",
      "| epoch   9 |  2200/ 2983 batches | lr 20.00 | ms/batch  8.31 | loss  4.45 | ppl    86.00\n",
      "| epoch   9 |  2400/ 2983 batches | lr 20.00 | ms/batch 10.15 | loss  4.50 | ppl    90.06\n",
      "| epoch   9 |  2600/ 2983 batches | lr 20.00 | ms/batch  9.19 | loss  4.54 | ppl    93.27\n",
      "| epoch   9 |  2800/ 2983 batches | lr 20.00 | ms/batch  8.52 | loss  4.47 | ppl    87.38\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 27.24s | valid loss  4.96 | valid ppl   142.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  10 |   200/ 2983 batches | lr 20.00 | ms/batch  8.28 | loss  4.53 | ppl    92.96\n",
      "| epoch  10 |   400/ 2983 batches | lr 20.00 | ms/batch  8.67 | loss  4.56 | ppl    95.29\n",
      "| epoch  10 |   600/ 2983 batches | lr 20.00 | ms/batch  8.41 | loss  4.39 | ppl    80.72\n",
      "| epoch  10 |   800/ 2983 batches | lr 20.00 | ms/batch  8.36 | loss  4.45 | ppl    85.48\n",
      "| epoch  10 |  1000/ 2983 batches | lr 20.00 | ms/batch  8.26 | loss  4.47 | ppl    87.64\n",
      "| epoch  10 |  1200/ 2983 batches | lr 20.00 | ms/batch  8.80 | loss  4.48 | ppl    88.43\n",
      "| epoch  10 |  1400/ 2983 batches | lr 20.00 | ms/batch  8.21 | loss  4.52 | ppl    92.19\n",
      "| epoch  10 |  1600/ 2983 batches | lr 20.00 | ms/batch  8.32 | loss  4.60 | ppl    99.38\n",
      "| epoch  10 |  1800/ 2983 batches | lr 20.00 | ms/batch  8.43 | loss  4.49 | ppl    88.98\n",
      "| epoch  10 |  2000/ 2983 batches | lr 20.00 | ms/batch  8.47 | loss  4.52 | ppl    92.25\n",
      "| epoch  10 |  2200/ 2983 batches | lr 20.00 | ms/batch  8.44 | loss  4.42 | ppl    82.97\n",
      "| epoch  10 |  2400/ 2983 batches | lr 20.00 | ms/batch  8.40 | loss  4.46 | ppl    86.62\n",
      "| epoch  10 |  2600/ 2983 batches | lr 20.00 | ms/batch  8.16 | loss  4.50 | ppl    89.78\n",
      "| epoch  10 |  2800/ 2983 batches | lr 20.00 | ms/batch  8.41 | loss  4.43 | ppl    84.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 26.53s | valid loss  4.96 | valid ppl   142.74\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Loop over epochs.\n",
    "lr = args.lr\n",
    "best_val_loss = None\n",
    "\n",
    "# At any point you can hit Ctrl + C to break out of training early.\n",
    "try:\n",
    "    for epoch in range(1, args.epochs+1):\n",
    "        epoch_start_time = time.time()\n",
    "        train()\n",
    "        val_loss = evaluate(val_data)\n",
    "        print('-' * 89)\n",
    "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                           val_loss, math.exp(val_loss)))\n",
    "        print('-' * 89)\n",
    "        # Save the model if the validation loss is the best we've seen so far.\n",
    "        if not best_val_loss or val_loss < best_val_loss:\n",
    "            with open(args.save, 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "            best_val_loss = val_loss\n",
    "        else:\n",
    "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "            lr /= 4.0\n",
    "except KeyboardInterrupt:\n",
    "    print('-' * 89)\n",
    "    print('Exiting from training early')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2600523-86c1-4f31-a000-719ccada54aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best saved model.\n",
    "with open(args.save, 'rb') as f:\n",
    "    model = torch.load(f)\n",
    "    # after load the rnn params are not a continuous chunk of memory\n",
    "    # this makes them a continuous chunk, and will speed up forward pass\n",
    "    # Currently, only rnn model supports flatten_parameters function.\n",
    "    if args.model in ['RNN_TANH', 'RNN_RELU', 'LSTM', 'GRU']:\n",
    "        model.rnn.flatten_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35178c17-8b40-4f28-b25c-649276216eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | test loss  4.89 | test ppl   133.20\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Run on test data.\n",
    "test_loss = evaluate(test_data)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
    "    test_loss, math.exp(test_loss)))\n",
    "print('=' * 89)\n",
    "\n",
    "if len(args.onnx_export) > 0:\n",
    "    # Export the model in ONNX format.\n",
    "    export_onnx(args.onnx_export, batch_size=1, seq_len=args.bptt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e723ff0c-9bd0-4404-9c54-efc9e825fe7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch Wikitext-2 Language Model')\n",
    "# Model parameters.\n",
    "parser.add_argument('--data', type=str, default='./data/wikitext-2',\n",
    "                    help='location of the data corpus')\n",
    "parser.add_argument('--checkpoint', type=str, default='./model.pt',\n",
    "                    help='model checkpoint to use')\n",
    "parser.add_argument('--outf', type=str, default='generated.txt',\n",
    "                    help='output file for generated text')\n",
    "parser.add_argument('--words', type=int, default='1000',\n",
    "                    help='number of words to generate')\n",
    "parser.add_argument('--seed', type=int, default=1111,\n",
    "                    help='random seed')\n",
    "parser.add_argument('--cuda', action='store_true',\n",
    "                    help='use CUDA')\n",
    "parser.add_argument('--mps', action='store_true', default=False,\n",
    "                        help='enables macOS GPU training')\n",
    "parser.add_argument('--temperature', type=float, default=1.0,\n",
    "                    help='temperature - higher will increase diversity')\n",
    "parser.add_argument('--log-interval', type=int, default=100,\n",
    "                    help='reporting interval')\n",
    "args = parser.parse_args(args=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd72c451-1353-4af1-8003-f47bb202c39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: You have a CUDA device, so you should probably run with --cuda.\n"
     ]
    }
   ],
   "source": [
    "# Set the random seed manually for reproducibility.\n",
    "torch.manual_seed(args.seed)\n",
    "if torch.cuda.is_available():\n",
    "    if not args.cuda:\n",
    "        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda.\")\n",
    "if torch.backends.mps.is_available():\n",
    "    if not args.mps:\n",
    "        print(\"WARNING: You have mps device, to enable macOS GPU run with --mps.\")\n",
    "        \n",
    "use_mps = args.mps and torch.backends.mps.is_available()\n",
    "if args.cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "elif use_mps:\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43538863-1f88-41f5-a3e0-bf36292b87b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNModel(\n",
       "  (drop): Dropout(p=0.2, inplace=False)\n",
       "  (encoder): Embedding(33278, 200)\n",
       "  (rnn): LSTM(200, 200, num_layers=2, dropout=0.2)\n",
       "  (decoder): Linear(in_features=200, out_features=33278, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if args.temperature < 1e-3:\n",
    "    parser.error(\"--temperature has to be greater or equal 1e-3.\")\n",
    "\n",
    "with open(args.checkpoint, 'rb') as f:\n",
    "    model = torch.load(f, map_location=device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a0732c7c-a54d-4659-88d0-b9a54c43ab81",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpus(args.data)\n",
    "ntokens = len(corpus.dictionary)\n",
    "\n",
    "is_transformer_model = hasattr(model, 'model_type') and model.model_type == 'Transformer'\n",
    "if not is_transformer_model:\n",
    "    hidden = model.init_hidden(1)\n",
    "input = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9a329eb-91d1-4a71-aa1d-951ae66a21dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Generated 0/1000 words\n",
      "| Generated 100/1000 words\n",
      "| Generated 200/1000 words\n",
      "| Generated 300/1000 words\n",
      "| Generated 400/1000 words\n",
      "| Generated 500/1000 words\n",
      "| Generated 600/1000 words\n",
      "| Generated 700/1000 words\n",
      "| Generated 800/1000 words\n",
      "| Generated 900/1000 words\n"
     ]
    }
   ],
   "source": [
    "with open(args.outf, 'w', encoding=\"utf8\") as outf:\n",
    "    with torch.no_grad():  # no tracking history\n",
    "        for i in range(args.words):\n",
    "            if is_transformer_model:\n",
    "                output = model(input, False)\n",
    "                word_weights = output[-1].squeeze().div(args.temperature).exp().cpu()\n",
    "                word_idx = torch.multinomial(word_weights, 1)[0]\n",
    "                word_tensor = torch.Tensor([[word_idx]]).long().to(device)\n",
    "                input = torch.cat([input, word_tensor], 0)\n",
    "            else:\n",
    "                output, hidden = model(input, hidden)\n",
    "                word_weights = output.squeeze().div(args.temperature).exp().cpu()\n",
    "                word_idx = torch.multinomial(word_weights, 1)[0]\n",
    "                input.fill_(word_idx)\n",
    "\n",
    "            word = corpus.dictionary.idx2word[word_idx]\n",
    "\n",
    "            outf.write(word + ('\\n' if i % 20 == 19 else ' '))\n",
    "\n",
    "            if i % args.log_interval == 0:\n",
    "                print('| Generated {}/{} words'.format(i, args.words))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
