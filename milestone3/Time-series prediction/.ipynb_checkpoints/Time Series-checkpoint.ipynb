{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPaua1FlUBTUtClDLysFj7L"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":9,"metadata":{"id":"UKuCYRPwPKTE","executionInfo":{"status":"ok","timestamp":1720356929228,"user_tz":-540,"elapsed":358,"user":{"displayName":"이주창","userId":"11438547241459994037"}}},"outputs":[],"source":["from __future__ import print_function\n","import argparse\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","import matplotlib\n","matplotlib.use('Agg')\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","source":["np.random.seed(2)\n","\n","T = 20\n","L = 1000\n","N = 100\n","\n","x = np.empty((N, L), 'int64')\n","x[:] = np.array(range(L)) + np.random.randint(-4 * T, 4 * T, N).reshape(N, 1)\n","data = np.sin(x / 1.0 / T).astype('float64')\n","torch.save(data, open('traindata.pt', 'wb'))"],"metadata":{"id":"epBJywYWPXCL","executionInfo":{"status":"ok","timestamp":1720356930608,"user_tz":-540,"elapsed":2,"user":{"displayName":"이주창","userId":"11438547241459994037"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["class Sequence(nn.Module):\n","    def __init__(self):\n","        super(Sequence, self).__init__()\n","        self.lstm1 = nn.LSTMCell(1, 51)\n","        self.lstm2 = nn.LSTMCell(51, 51)\n","        self.linear = nn.Linear(51, 1)\n","\n","    def forward(self, input, future = 0):\n","        outputs = []\n","        h_t = torch.zeros(input.size(0), 51, dtype=torch.double)\n","        c_t = torch.zeros(input.size(0), 51, dtype=torch.double)\n","        h_t2 = torch.zeros(input.size(0), 51, dtype=torch.double)\n","        c_t2 = torch.zeros(input.size(0), 51, dtype=torch.double)\n","\n","        for input_t in input.split(1, dim=1):\n","            h_t, c_t = self.lstm1(input_t, (h_t, c_t))\n","            h_t2, c_t2 = self.lstm2(h_t, (h_t2, c_t2))\n","            output = self.linear(h_t2)\n","            outputs += [output]\n","        for i in range(future):# if we should predict the future\n","            h_t, c_t = self.lstm1(output, (h_t, c_t))\n","            h_t2, c_t2 = self.lstm2(h_t, (h_t2, c_t2))\n","            output = self.linear(h_t2)\n","            outputs += [output]\n","        outputs = torch.cat(outputs, dim=1)\n","        return outputs"],"metadata":{"id":"lADR_ZJKPOf2","executionInfo":{"status":"ok","timestamp":1720356933315,"user_tz":-540,"elapsed":355,"user":{"displayName":"이주창","userId":"11438547241459994037"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["if __name__ == '__main__':\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument('--steps', type=int, default=15, help='steps to run')\n","    opt = parser.parse_args(args=\"\")\n","    # set random seed to 0\n","    np.random.seed(0)\n","    torch.manual_seed(0)\n","    # load data and make training set\n","    data = torch.load('traindata.pt')\n","    input = torch.from_numpy(data[3:, :-1])\n","    target = torch.from_numpy(data[3:, 1:])\n","    test_input = torch.from_numpy(data[:3, :-1])\n","    test_target = torch.from_numpy(data[:3, 1:])\n","    # build the model\n","    seq = Sequence()\n","    seq.double()\n","    criterion = nn.MSELoss()\n","    # use LBFGS as optimizer since we can load the whole data to train\n","    optimizer = optim.LBFGS(seq.parameters(), lr=0.8)\n","    #begin to train\n","    for i in range(opt.steps):\n","        print('STEP: ', i)\n","        def closure():\n","            optimizer.zero_grad()\n","            out = seq(input)\n","            loss = criterion(out, target)\n","            print('loss:', loss.item())\n","            loss.backward()\n","            return loss\n","        optimizer.step(closure)\n","        # begin to predict, no need to track gradient here\n","        with torch.no_grad():\n","            future = 1000\n","            pred = seq(test_input, future=future)\n","            loss = criterion(pred[:, :-future], test_target)\n","            print('test loss:', loss.item())\n","            y = pred.detach().numpy()\n","        # draw the result\n","        plt.figure(figsize=(30,10))\n","        plt.title('Predict future values for time sequences\\n(Dashlines are predicted values)', fontsize=30)\n","        plt.xlabel('x', fontsize=20)\n","        plt.ylabel('y', fontsize=20)\n","        plt.xticks(fontsize=20)\n","        plt.yticks(fontsize=20)\n","        def draw(yi, color):\n","            plt.plot(np.arange(input.size(1)), yi[:input.size(1)], color, linewidth = 2.0)\n","            plt.plot(np.arange(input.size(1), input.size(1) + future), yi[input.size(1):], color + ':', linewidth = 2.0)\n","        draw(y[0], 'r')\n","        draw(y[1], 'g')\n","        draw(y[2], 'b')\n","        plt.savefig('predict%d.pdf'%i)\n","        plt.close()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6-CWhpC4PTTu","executionInfo":{"status":"ok","timestamp":1720357710894,"user_tz":-540,"elapsed":775324,"user":{"displayName":"이주창","userId":"11438547241459994037"}},"outputId":"b2f5ac50-724f-4a88-c1c3-1e94ce3e9fa4"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["STEP:  0\n","loss: 0.5023738120466186\n","loss: 0.49856639324547236\n","loss: 0.47901196020146203\n","loss: 0.44633490395179287\n","loss: 0.3540631027261265\n","loss: 0.20507018018165263\n","loss: 1.3960544675936566\n","loss: 0.03249442160928474\n","loss: 0.02993487897293396\n","loss: 0.028326823118628365\n","loss: 0.026830615757894825\n","loss: 0.023771207077302403\n","loss: 0.01890142131510013\n","loss: 0.010646824595743933\n","loss: 0.008725753397927041\n","loss: 0.007872182058727216\n","loss: 0.00547784397325814\n","loss: 0.004051934991144172\n","loss: 0.0027296270509008535\n","loss: 0.0015402697323593625\n","test loss: 0.001300089410189388\n","STEP:  1\n","loss: 0.0012797663945830202\n","loss: 0.0011690563508991146\n","loss: 0.0011498925364591239\n","loss: 0.0011288249265203364\n","loss: 0.001063054928950748\n","loss: 0.0009563977558898084\n","loss: 0.0008210794177760191\n","loss: 0.0007670824741781775\n","loss: 0.00072947329603916\n","loss: 0.0007246574843768429\n","loss: 0.0007206232232253409\n","loss: 0.000712671248819364\n","loss: 0.0006961347274588448\n","loss: 0.0006641155429860777\n","loss: 0.0006101283738641806\n","loss: 0.0005285312572236519\n","loss: 0.000412734228852603\n","loss: 0.0003302206531781029\n","loss: 0.00031216901261872605\n","loss: 0.00032349338741358174\n","test loss: 0.0001700500660213029\n","STEP:  2\n","loss: 0.0003053078224621829\n","loss: 0.0003041505055391082\n","loss: 0.00030351429723215144\n","loss: 0.00030276621743043736\n","loss: 0.000301032008061224\n","loss: 0.0002974385385361584\n","loss: 0.00029075562500353294\n","loss: 0.00028094065220486924\n","loss: 0.00026821077806399654\n","loss: 0.0002522027838089772\n","loss: 0.00023918952177334182\n","loss: 0.00022589596695958065\n","loss: 0.00022718148539635543\n","loss: 0.00020803625211565185\n","loss: 0.00020052777117108162\n","loss: 0.00019150777052628855\n","loss: 0.00018778247917964372\n","loss: 0.00018118456272883218\n","loss: 0.00017654822382728446\n","loss: 0.00016929999893043943\n","test loss: 6.581389448880544e-05\n","STEP:  3\n","loss: 0.00015964540896124275\n","loss: 0.00015392533728655458\n","loss: 0.00015169599500788078\n","loss: 0.00015151050072412969\n","loss: 0.00015118914851566868\n","loss: 0.0001510721718100064\n","loss: 0.00015086189701985257\n","loss: 0.00015074319072552492\n","loss: 0.0001506169799648209\n","loss: 0.00015048849896669557\n","loss: 0.00015031072770755355\n","loss: 0.0001500234758032725\n","loss: 0.0001495209124262363\n","loss: 0.0001487312245562469\n","loss: 0.00014718181190369006\n","loss: 0.0001447828920275907\n","loss: 0.0001407382233610761\n","loss: 0.0001322382161380334\n","loss: 0.00012006753879163612\n","loss: 0.00010807475167843554\n","test loss: 4.9394383564690916e-05\n","STEP:  4\n","loss: 0.00010530505450465149\n","loss: 9.854286828485415e-05\n","loss: 9.612525286982468e-05\n","loss: 9.211327683101353e-05\n","loss: 8.66605467111451e-05\n","loss: 8.364259504020775e-05\n","loss: 7.986642301041872e-05\n","loss: 7.815958464916581e-05\n","loss: 7.566096986084814e-05\n","loss: 7.601186336936099e-05\n","loss: 7.426579474222176e-05\n","loss: 7.395040406025344e-05\n","loss: 7.381224027009339e-05\n","loss: 7.352003444749438e-05\n","loss: 7.329674235920176e-05\n","loss: 7.277814895532728e-05\n","loss: 7.156687565554338e-05\n","loss: 6.9884920111761e-05\n","loss: 6.894773039267256e-05\n","loss: 6.905277988295134e-05\n","test loss: 4.1516720362209696e-05\n","STEP:  5\n","loss: 6.865591112060171e-05\n","loss: 6.862719091836474e-05\n","loss: 6.85229826335069e-05\n","loss: 6.838521931309934e-05\n","loss: 6.80564273855527e-05\n","loss: 6.742908227157744e-05\n","loss: 6.660795094325065e-05\n","loss: 6.407666304836574e-05\n","loss: 6.276976170858902e-05\n","loss: 5.8945635362704654e-05\n","loss: 5.4652976734350136e-05\n","loss: 0.00021460303029423363\n","loss: 0.00016834454942659367\n","loss: 0.6211279743203639\n","loss: 189550.41837428528\n","loss: 20863.155471335336\n","loss: 41266149839.45485\n","loss: 11038641354002.79\n","loss: 1500822205746.0354\n","loss: 100463344585271.31\n","test loss: 1757541921733172.0\n","STEP:  6\n","loss: 1665458450753070.8\n","loss: 1822636117530694.2\n","loss: 5.8843158428481e+20\n","loss: 5.037693138512026e+23\n","loss: 1.7856209836597748e+22\n","loss: 1.9955555352891468e+26\n","loss: 4.96874018592848e+24\n","loss: 1.9209491867935668e+26\n","loss: 8.593703349503403e+27\n","loss: 2.1053987154514272e+27\n","loss: 2.816750166522471e+30\n","loss: 7.820027561778052e+28\n","loss: 3.899787823916924e+28\n","loss: 3.0629277888487443e+29\n","loss: 1.9380443031522133e+30\n","loss: 2.176936183099859e+30\n","loss: 2.6639504954707693e+30\n","loss: 7.896970737097536e+30\n","loss: 6.974858381644235e+29\n","loss: 8.392125023155589e+29\n","test loss: 8.663695764537303e+29\n","STEP:  7\n","loss: 1.0395786229380989e+30\n","loss: 1.2970141883490077e+31\n","loss: 3.521153729603551e+29\n","loss: 5.340211885720311e+28\n","loss: 3.052377510884736e+28\n","loss: 1.5336598782396365e+28\n","loss: 1.7844021337149326e+28\n","loss: 1.3941878472349094e+28\n","loss: 3.345252189345665e+28\n","loss: 2.0391216244615527e+28\n","loss: 2.0384761321181928e+28\n","loss: 7.290654622938454e+27\n","loss: 5.31462720384759e+27\n","loss: 5.447398520996373e+27\n","loss: 5.949951056913933e+27\n","loss: 9.326810908383881e+29\n","loss: 9.967695257168775e+29\n","loss: 3.313310724818842e+32\n","loss: 4.405616625245199e+29\n","loss: 1.2805617267487698e+29\n","test loss: 1.252757130933133e+29\n","STEP:  8\n","loss: 1.242989468561381e+29\n","loss: 1.781602368441422e+29\n","loss: 8.2041811282924e+29\n","loss: 2.1663454431170377e+29\n","loss: 2.3697376154873433e+29\n","loss: 1.2061615610673637e+29\n","loss: 5.854204606654363e+28\n","loss: 2.9905557524796355e+29\n","loss: 4.6519775489039125e+28\n","loss: 3.74962773293573e+28\n","loss: 3.1555658144844827e+28\n","loss: 2.416683526090602e+28\n","loss: 2.5030098492150936e+28\n","loss: 1.4170984640641643e+28\n","loss: 8.60859264991776e+27\n","loss: 3.983127869741355e+28\n","loss: 4.3727462106694255e+28\n","loss: 5.9231119214684484e+29\n","loss: 2.9916214851111252e+29\n","loss: 4.3730408059600957e+27\n","test loss: 3.545962958778371e+27\n","STEP:  9\n","loss: 3.0153844399462897e+27\n","loss: 7.2129789690749775e+31\n","loss: 1.3489263952108992e+30\n","loss: 1.0256163233880188e+29\n","loss: 7.322313352130001e+28\n","loss: 2.9116659757147436e+28\n","loss: 1.8768291158166886e+28\n","loss: 2.0238273042660207e+27\n","loss: 4.3613494527052876e+26\n","loss: 6.246387475549643e+26\n","loss: 4.256494211481381e+26\n","loss: 6.234599152248912e+26\n","loss: 9.134091450394138e+26\n","loss: 1.8982208284132114e+27\n","loss: 2.371171294155728e+27\n","loss: 3.0574199129226947e+28\n","loss: 1.1592236716808175e+33\n","loss: 3.1243939363420184e+31\n","loss: 9.494757631378911e+30\n","loss: 9.339111977400267e+30\n","test loss: 4.539037795973427e+30\n","STEP:  10\n","loss: 4.5785718549417106e+30\n","loss: 6.532550779097371e+29\n","loss: 4.3743434316536594e+29\n","loss: 1.6531832127141405e+29\n","loss: 1.6844839838919944e+29\n","loss: 1.0593172369829531e+30\n","loss: 3.0488513277615428e+29\n","loss: 1.4688991023028052e+29\n","loss: 1.471131897271177e+29\n","loss: 6.982099934558149e+28\n","loss: 5.715024669829249e+28\n","loss: 5.946840619615384e+28\n","loss: 2.1411718379119025e+29\n","loss: 2.4371648285098067e+29\n","loss: 1.4189320229898204e+29\n","loss: 4.958552466788617e+28\n","loss: 4.773250634385514e+28\n","loss: 8.654275427683222e+28\n","loss: 6.159168738492177e+28\n","loss: 1.2971928914483897e+29\n","test loss: 1.0659076078816891e+29\n","STEP:  11\n","loss: 1.0323358565935812e+29\n","loss: 1.4690851888233374e+29\n","loss: 3.0651175469439422e+29\n","loss: 7.778957951501729e+28\n","loss: 6.69004033738627e+28\n","loss: 4.8425130459433475e+28\n","loss: 9.161422043565476e+28\n","loss: 8.732120960678548e+28\n","loss: 7.634512815921876e+29\n","loss: 4.000534828612949e+29\n","loss: 9.072425976962744e+29\n","loss: 7.51323845444453e+28\n","loss: 7.047877411383014e+28\n","loss: 3.714318003371457e+28\n","loss: 2.845488788247294e+29\n","loss: 9.553037938170048e+29\n","loss: 3.267960343006789e+30\n","loss: 3.8238055326420046e+29\n","loss: 8.135538024405796e+29\n","loss: 8.835801471634314e+29\n","test loss: 1.489312779288769e+30\n","STEP:  12\n","loss: 1.511575145818838e+30\n","loss: 1.4471735382727067e+32\n","loss: 4.634462012613618e+30\n","loss: 9.802945147957328e+29\n","loss: 3.2675250660532826e+29\n","loss: 2.2276902715138006e+29\n","loss: 7.638153828078943e+29\n","loss: 7.871591860720007e+29\n","loss: 7.995919394595698e+28\n","loss: 1.4127920495972623e+29\n","loss: 1.525987864714903e+29\n","loss: 4.126717308120966e+29\n","loss: 9.938494707569174e+34\n","loss: 4.1364233044746716e+33\n","loss: 7.18164610578994e+31\n","loss: 4.8882867749452676e+30\n","loss: 1.7561066253640767e+30\n","loss: 1.6479863912899255e+30\n","loss: 7.119232030296337e+28\n","loss: 1.9042655104255538e+28\n","test loss: 9.782014006391217e+28\n","STEP:  13\n","loss: 9.829658426178066e+28\n","loss: 9.122982571131679e+29\n","loss: 4.32198099255473e+29\n","loss: 4.139734949721839e+29\n","loss: 5.3456811349509305e+29\n","loss: 4.771365859005441e+29\n","loss: 1.5877862857534937e+30\n","loss: 6.469825788777955e+30\n","loss: 4.7785040559375886e+29\n","loss: 1.1887594058832789e+30\n","loss: 2.8990911240056185e+30\n","loss: 1.6602855014515765e+40\n","loss: 6.591123671213589e+38\n","loss: 3.0083089738240286e+37\n","loss: 3.086927802785068e+36\n","loss: 2.5294875759047574e+38\n","loss: 2.487123192323047e+37\n","loss: 4.569984860635228e+36\n","loss: 5.940179983476879e+35\n","loss: 5.103595517129647e+35\n","test loss: 5.673507573580571e+36\n","STEP:  14\n","loss: 1.0494126825730401e+37\n","loss: 4.408041605825279e+35\n","loss: 1.0654977334536001e+35\n","loss: 8.1824530279543055e+34\n","loss: 5.0632174721660505e+34\n","loss: 6.069292242686147e+34\n","loss: 1.8949750981986855e+37\n","loss: 8.014604656783604e+35\n","loss: 1.3557363891713083e+35\n","loss: 1.3867309297249801e+35\n","loss: 1.5751620042471352e+35\n","loss: 4.570671153791073e+34\n","loss: 3.2643362445115077e+34\n","loss: 3.4802003614455965e+34\n","loss: 3.3165506083793715e+34\n","loss: 3.673483253336916e+34\n","loss: 5.083004046369487e+35\n","loss: 2.3819854404269204e+41\n","loss: 9.435296782177597e+39\n","loss: 5.687877032922788e+38\n","test loss: 7.570861263638691e+37\n"]}]}]}